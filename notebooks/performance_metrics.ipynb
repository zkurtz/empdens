{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the performance of a density estimator\n",
    "\n",
    "Define a density estimator as *proper* if it always generates a true density function, integrating to 1 over the entire feature space. Most density estimators in the literature are proper at least on paper, but can still turn out to be improper in practice due to numerical issues or simplifications during implementation.\n",
    "\n",
    "The optimal strategy for evaluating a density estimate depends on the use case, whether the generative (i.e. *ground truth*) density is known, and whether the estimator is proper.\n",
    "\n",
    "\n",
    "## 1. Unknown generative density\n",
    "\n",
    "Assume that the generative density is unknown (hence the need to do density estimation). Then there are several methods of evaluating performance.\n",
    "\n",
    "### 1.1 Derive expressions for integrated squared error in cross-validation\n",
    "\n",
    "[This lecture by Larry Wasserman](http://www.stat.cmu.edu/~larry/=sml/densityestimation.pdf)\n",
    "starts from the integrated squared error \n",
    "$$ \\int ( \\hat p(x) - p(x) )^2 dx $$\n",
    "to motivate loss function\n",
    "$$ \\int \\hat p(x)^2 dx - \\frac{2}{n} \\sum_{i=1}^n \\hat p(x_i) $$\n",
    "where $x_1, ..., x_n$ is a hold-out falidation set. The second term is trivial but the first term requires computing an integral. This turns out to be tractible for histograms and kernel density estimators. TODO: extend this to tree-based estimators?\n",
    "\n",
    "### 1.2 Cross-validation via the log likelihood\n",
    "\n",
    "*BLUF: Attractive because ... cross validation! - but use with caution.* \n",
    "\n",
    "Given a proper density estimator, the mean log \\[estimated\\] density, or simply the log likelihood, over a hold-out dataset is a well-known method of performance evaluation (for example, see [Schmidberger and Frank](https://link.springer.com/content/pdf/10.1007/11564126_26.pdf)). Given hold-out data $x = (x_1, ..., x_n)$ and some estimated density function $\\hat p$, the log likelihood is\n",
    "\n",
    "$$\\sum_i \\log \\hat p (x_i) $$\n",
    "\n",
    "This is a direct estimate of the part of the KL divergence $\\int p(x) \\log \\frac{p(x)}{\\hat p_x} dx$ that depends on $\\hat p$. Intuitively, a high value of the likelihood means that the estimator figured out exactly where the high-density regions are and placed relatively high likelihood on data points appearing in those locations. \n",
    "\n",
    "Although cross-validation on the log likelihood is attractive for its simplicity, two major caveats apply:\n",
    "- [Hall (1987)](https://projecteuclid.org/download/pdf_1/euclid.aos/1176350606) showed that this approach can lead to \"infinite loss and inconsistent estimation\", depending in the tail distributions for the target density and density estimator.\n",
    "- This approach is relevant only for proper estimators. If the density is allowed to be improper, it's trivial to game the metric simply by assigning a very high density estimate to every point.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Known generative density\n",
    "\n",
    "Evaluating an estimator in terms of the generative density gives an indirect way to judge the performance of the estimator on a real data set: To the extent that you believe the simulation data is like the real data, then you might expect performance on the real data to be comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Integrated L1 and L2 losses\n",
    "\n",
    "[Deng and Wickham](https://vita.had.co.nz/papers/density-estimation.pdf) used a mean absolute error to compare estimated densities against the density used to simulate the data.\n",
    "\n",
    "...\n",
    "\n",
    "...\n",
    "TODO: pull this together\n",
    "...\n",
    "\n",
    "...\n",
    "\n",
    "The integrated L1 and L2 losses are not the same as the expectations of the L1 and L2 losses, respectively. Given a hold-out sample $x_1, ..., x_n$, \n",
    "- $\\frac{1}{n} \\sum_i | \\hat p(x_i) - p(x_i)| $ is a direct estimate of $\\int |\\hat p(x) - p(x)| p(x) dx$, the expectation of L2 loss. Similarly, \n",
    "- $\\frac{1}{n} \\sum_i (\\hat p(x_i) - p(x_i))^2$ is a direct estimate of $\\int (\\hat p(x) - p(x))^2 p(x) dx$, the expectation of L1 loss.\n",
    "\n",
    "Both of these differ from the integrated L1 and L2 losses:\n",
    "- L1: $\\int |\\hat p(x) - p(x)| dx$\n",
    "- L2: $\\int (\\hat p(x) - p(x))^2 dx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Correlation\n",
    "\n",
    "Computing the correlation between (a) the density estimate on a hold out test set and (b) the \\[true\\] generative density evaluated at the test points is a non-standard way of evaluating a density estimate..\n",
    "Two kinds of correlation are of particular interest:\n",
    "- The Spearman (rank-order) correlation; a density estimate attains the maximum (1) whenever it ranks the test points in the same order as the true density\n",
    "- Pearson (the usual) correlation; a density estimate attains the maximum (1) whenever it is of the form $\\hat p(x) = m p(x) + b$ for some positive constant $m$ and scalar $b$.\n",
    "\n",
    "Both of these correlations are \n",
    "\n",
    "Correlation is particularly attractive for its ability to evaluate improper density estimates. Indeed, the properness of a density estimate is often of no practical significance in real-world use cases. For example, an \n",
    "[isolation forest](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf?q=isolation-forest) \n",
    "assigns high values to points of high density. Although though this function is not a proper density, it can be used for anomaly detection or mode detection just as any density estimate. We'd like a success metric that puts the isolation forest on equal footing with a proper density estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional references\n",
    "\n",
    "- In [his lecture](http://www.stat.cmu.edu/~larry/=sml/densityestimation.pdf), Wasserman highlights [Devroye and Gyorfi (1985)](http://luc.devroye.org/L1bookBW.pdf), endorsing integrated L1 loss due to interpretability and invariance under certain transformations\n",
    "- [Klein and Richardson](http://www.stat.cmu.edu/~lrichard/links/density_trees.pdf)\n",
    "review several theoretical properties of piecewise constant density estimators in terms of the integrated squared error and related metrics.\n",
    "- [Seaman and Powell](https://www.researchgate.net/publication/224817410_An_Evaluation_of_the_Accuracy_of_Kernel_Density_Estimators_for_Home_Range_Analysis) use some kind of MISE (although the $f(x)$ in their denominator seems to conflict with relatively modern MISE definitions)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
